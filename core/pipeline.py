"""
Event pipeline for JARVIS ‚Äî Phase 4 of the latency refactor.

Provides queue-connected worker threads and a coordinator that replaces
the ad-hoc callback architecture with a centralized event dispatch loop.

Components:
    Coordinator   ‚Äî main-thread event loop, routes commands, manages state
    STTWorker     ‚Äî persistent transcription thread (replaces per-utterance daemons)
    TTSWorker     ‚Äî persistent playback thread (serializes all audio output)
    EventBridge   ‚Äî adapter that translates callback-based APIs into events
    EventTTSProxy ‚Äî drop-in TTS replacement for background services
"""

import queue
import re
import threading
import time
import logging
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher
from typing import Optional

from core.events import Event, EventType, PipelineState
from core.speech_chunker import SpeechChunker
from core.logger import get_logger
from core.honorific import set_honorific
from core import persona
from core.conversation_state import ConversationState
from core.conversation_router import ConversationRouter, RouteResult
from core.llm_router import ToolCallRequest
from core.web_research import WebResearcher, format_search_results


# ---------------------------------------------------------------------------
# STT Worker
# ---------------------------------------------------------------------------

class STTWorker(threading.Thread):
    """Persistent STT worker. Reads audio from a queue, transcribes,
    and emits TRANSCRIPTION_READY events to the coordinator.

    When a SpeakerIdentifier is provided, speaker identification runs
    in parallel with Whisper transcription via a ThreadPoolExecutor.
    """

    def __init__(self, stt, event_queue: queue.Queue, audio_queue: queue.Queue,
                 config=None, speaker_id=None):
        super().__init__(daemon=True, name="stt-worker")
        self.stt = stt
        self.event_queue = event_queue
        self.audio_queue = audio_queue
        self.speaker_id = speaker_id
        self.logger = get_logger("pipeline.stt", config)

    def run(self):
        self.logger.info("STT worker started")
        while True:
            audio = self.audio_queue.get()
            if audio is None:  # shutdown sentinel
                self.logger.info("STT worker shutting down")
                break
            try:
                sample_rate = 16000  # audio is always resampled to 16 kHz

                if self.speaker_id is not None:
                    # Run Whisper + speaker ID in parallel
                    with ThreadPoolExecutor(max_workers=2) as pool:
                        stt_future = pool.submit(self.stt.transcribe, audio, sample_rate)
                        sid_future = pool.submit(self.speaker_id.identify, audio, sample_rate)
                        text = stt_future.result()
                        speaker_user_id, speaker_confidence = sid_future.result()
                else:
                    text = self.stt.transcribe(audio, sample_rate)
                    speaker_user_id, speaker_confidence = None, 0.0

                if text and text.strip():
                    # Enriched event data when speaker ID is available
                    if self.speaker_id is not None:
                        data = {
                            "text": text.strip(),
                            "speaker_id": speaker_user_id,
                            "speaker_confidence": speaker_confidence,
                        }
                    else:
                        data = text.strip()

                    self.event_queue.put(Event(
                        EventType.TRANSCRIPTION_READY,
                        data=data,
                        source="stt_worker",
                    ))
                else:
                    self.logger.info("Blank transcription")
                    print("‚ö†Ô∏è  (no speech detected)")
            except Exception as e:
                self.logger.error(f"STT worker error: {e}", exc_info=True)
                self.event_queue.put(Event(
                    EventType.ERROR,
                    data={"source": "stt", "error": str(e)},
                    source="stt_worker",
                ))


# ---------------------------------------------------------------------------
# TTS Worker
# ---------------------------------------------------------------------------

class TTSWorker(threading.Thread):
    """Persistent TTS worker. Reads speak requests from a queue and
    plays them sequentially, emitting lifecycle events."""

    def __init__(self, tts, event_queue: queue.Queue, tts_queue: queue.Queue,
                 config=None):
        super().__init__(daemon=True, name="tts-worker")
        self.tts = tts
        self.event_queue = event_queue
        self.tts_queue = tts_queue
        self.logger = get_logger("pipeline.tts", config)

    def run(self):
        self.logger.info("TTS worker started")
        while True:
            item = self.tts_queue.get()
            if item is None:  # shutdown sentinel
                self.logger.info("TTS worker shutting down")
                break

            event = item
            done_event = None  # threading.Event for synchronous callers

            # Emit pause + started
            self.event_queue.put(Event(EventType.PAUSE_LISTENING, source="tts_worker"))
            self.event_queue.put(Event(EventType.SPEECH_STARTED, source="tts_worker"))

            try:
                if event.type == EventType.SPEAK_ACK:
                    self.tts.speak_ack()
                elif event.type == EventType.SPEAK_REQUEST:
                    data = event.data
                    if isinstance(data, dict):
                        done_event = data.get("done_event")
                        text = data.get("text", "")
                    else:
                        text = str(data)
                    if text:
                        self.tts.speak(text)
            except Exception as e:
                self.logger.error(f"TTS worker error: {e}", exc_info=True)
                self.event_queue.put(Event(
                    EventType.ERROR,
                    data={"source": "tts", "error": str(e)},
                    source="tts_worker",
                ))

            # Emit finished
            self.event_queue.put(Event(EventType.SPEECH_FINISHED, source="tts_worker"))

            # Signal synchronous callers (EventTTSProxy)
            if done_event is not None:
                done_event.set()


# ---------------------------------------------------------------------------
# EventBridge ‚Äî adapter for background services' listener callbacks
# ---------------------------------------------------------------------------

class EventBridge:
    """Translates callback-based interactions into events.

    Background services (reminder_manager, news_manager) were designed to
    call listener.pause_listening() / resume_listening() directly.  This
    adapter provides the same API but emits events instead, so the
    coordinator can manage all listener state centrally.
    """

    def __init__(self, event_queue: queue.Queue):
        self.event_queue = event_queue

    def pause_listening(self):
        self.event_queue.put(Event(EventType.PAUSE_LISTENING, source="bridge"))

    def resume_listening(self):
        self.event_queue.put(Event(EventType.RESUME_LISTENING, source="bridge"))

    def open_conversation_window(self, duration: float = None):
        self.event_queue.put(Event(
            EventType.OPEN_CONVERSATION_WINDOW,
            data=duration,
            source="bridge",
        ))


# ---------------------------------------------------------------------------
# EventTTSProxy ‚Äî drop-in TTS for background services
# ---------------------------------------------------------------------------

class EventTTSProxy:
    """Drop-in TTS replacement that routes through the TTS worker queue.

    Background services (reminder_manager, news_manager) hold a reference
    to this instead of the real TTS.  speak() blocks until playback
    finishes, preserving the synchronous contract these services expect.
    """

    def __init__(self, tts_queue: queue.Queue, event_queue: queue.Queue):
        self.tts_queue = tts_queue
        self.event_queue = event_queue
        self._spoke = False

    # --- public API matching core.tts.TextToSpeech ---

    def speak(self, text: str):
        """Speak text via the TTS worker.  Blocks until playback finishes."""
        self._spoke = True
        done = threading.Event()
        self.tts_queue.put(Event(
            EventType.SPEAK_REQUEST,
            data={"text": text, "done_event": done},
            source="bg_service",
        ))
        done.wait(timeout=60)

    def speak_ack(self):
        """Play a pre-cached acknowledgment phrase (non-blocking)."""
        self.tts_queue.put(Event(EventType.SPEAK_ACK, source="bg_service"))


# ---------------------------------------------------------------------------
# StreamingAudioPipeline ‚Äî gapless multi-sentence TTS
# ---------------------------------------------------------------------------

class StreamingAudioPipeline:
    """Background audio pipeline for gapless multi-sentence TTS.

    Accepts sentence text via put(), generates audio via Kokoro,
    and streams PCM to a single persistent aplay process.
    Eliminates inter-sentence gaps by overlapping generation with playback.
    """

    def __init__(self, tts, logger):
        self.tts = tts
        self.logger = logger
        self._text_queue = queue.Queue()
        self._done = threading.Event()
        self._error = None
        self._total_chunks = 0
        self._thread = None

    def start(self):
        """Start the background audio pipeline thread."""
        self._done.clear()
        self._error = None
        self._total_chunks = 0
        self._thread = threading.Thread(
            target=self._run, daemon=True, name="streaming-audio"
        )
        self._thread.start()

    def put(self, text: str):
        """Submit a sentence for audio generation and playback."""
        self._text_queue.put(text)
        self._total_chunks += 1

    def finish(self):
        """Signal no more sentences. Blocks until all audio finishes."""
        self._text_queue.put(None)  # sentinel
        self._done.wait(timeout=120)
        if self._error:
            self.logger.error(f"Streaming audio pipeline error: {self._error}")

    def _run(self):
        """Background thread: generate audio and stream to persistent aplay."""
        import subprocess
        import numpy as np

        tts = self.tts
        aplay = None
        total_samples = 0
        t0 = time.time()
        first_chunk_logged = False

        try:
            with tts._tts_lock:
                while True:
                    text = self._text_queue.get()
                    if text is None:
                        break

                    # Normalize
                    if tts.normalization_enabled and tts.normalizer:
                        text = tts.normalizer.normalize(text)

                    if not text or not text.strip():
                        continue

                    # Stream Kokoro sub-chunks directly to aplay
                    for gs, ps, audio in tts._kokoro_pipeline(
                        text, voice=tts._kokoro_voice,
                        speed=tts._kokoro_speed
                    ):
                        audio_np = np.asarray(audio)
                        pcm = (audio_np * 32767).astype(
                            np.int16
                        ).tobytes()

                        # Lazy-spawn aplay on first audio data
                        # (not first sentence ‚Äî gives PipeWire
                        # Kokoro-generation time to release device)
                        if aplay is None:
                            aplay = tts._open_aplay()
                            if aplay is None:
                                self._error = "Failed to open audio device"
                                break
                            tts._track_proc(aplay)

                        aplay.stdin.write(pcm)
                        total_samples += len(audio_np)

                        if not first_chunk_logged:
                            first_chunk_logged = True
                            self.logger.info(
                                f"Kokoro first chunk in "
                                f"{time.time() - t0:.3f}s"
                            )

                    if self._error:
                        break

                # All sentences done ‚Äî close aplay
                if aplay is not None:
                    aplay.stdin.close()
                    duration = total_samples / tts.sample_rate
                    gen_time = time.time() - t0

                    try:
                        aplay_return = aplay.wait(
                            timeout=max(15, duration + 5)
                        )
                    except subprocess.TimeoutExpired:
                        self.logger.error("aplay timed out ‚Äî killing")
                        aplay.kill()
                        aplay.wait()
                        return

                    if aplay_return != 0:
                        aplay_err = aplay.stderr.read().decode().strip()
                        self.logger.error(
                            f"aplay error (code {aplay_return}): "
                            f"{aplay_err}"
                        )
                    else:
                        self.logger.info(
                            f"Kokoro streamed {duration:.1f}s audio in "
                            f"{gen_time:.3f}s across "
                            f"{self._total_chunks} chunks "
                            f"(RTF: {duration/gen_time:.1f}x)"
                        )

        except BrokenPipeError:
            if aplay:
                aplay_err = aplay.stderr.read().decode().strip()
                self.logger.error(f"aplay broken pipe: {aplay_err}")
                aplay.wait()
            self._error = "aplay broken pipe"

        except Exception as e:
            self.logger.error(f"Streaming audio pipeline error: {e}")
            import traceback
            traceback.print_exc()
            self._error = str(e)
            if aplay and aplay.poll() is None:
                try:
                    aplay.stdin.close()
                except Exception:
                    pass
                aplay.kill()
                aplay.wait()

        finally:
            if aplay is not None:
                tts._untrack_proc(aplay)
            self._done.set()


# ---------------------------------------------------------------------------
# Coordinator ‚Äî main-thread event loop
# ---------------------------------------------------------------------------

class Coordinator:
    """Central event dispatcher running on the main thread.

    Replaces the old ``while running: sleep(0.1)`` loop.  Receives typed
    events from all workers and background services, makes routing
    decisions, and manages conversation state.
    """

    def __init__(self, *, config, event_queue: queue.Queue,
                 tts_queue: queue.Queue, listener, tts, llm,
                 skill_manager, conversation, reminder_manager=None,
                 news_manager=None, calendar_manager=None,
                 profile_manager=None, memory_manager=None,
                 context_window=None, desktop_manager=None):
        self.config = config
        self.logger = get_logger("pipeline.coordinator", config)
        self.event_queue = event_queue
        self.tts_queue = tts_queue
        self.listener = listener
        self.tts = tts
        self.llm = llm
        self.skill_manager = skill_manager
        self.conversation = conversation
        self.reminder_manager = reminder_manager
        self.news_manager = news_manager
        self.calendar_manager = calendar_manager
        self.profile_manager = profile_manager
        self.memory_manager = memory_manager
        self.context_window = context_window
        self.desktop_manager = desktop_manager

        # Web research (tool calling)
        self.web_researcher = WebResearcher(config) if config.get("llm.local.tool_calling", False) else None

        # Centralized conversation state (Phase 2 of conversational flow refactor)
        self.conv_state = ConversationState()

        self.running = True
        self.state = PipelineState.IDLE
        self.wake_word = config.get("system.wake_word", "jarvis").lower()

        # Session stats for health reporting
        self.stats = {
            'start_time': time.time(),
            'commands_processed': 0,
            'errors': 0,
            'last_error_time': None,
            'last_error_msg': None,
        }

        # Streaming LLM state
        self._streaming_active = False
        self._llm_responded = False

        # Beep
        from pathlib import Path
        self.beep_path = Path(__file__).parent.parent / "assets" / "wake_word_detect.wav"

        # Valid short replies (copied from continuous_listener for conversation noise filter)
        self._valid_short_replies = {
            "yes", "no", "yeah", "yep", "nah", "nope",
            "thanks", "thank you", "okay", "ok", "please",
            "stop", "cancel", "nevermind", "never mind",
            "sure", "right", "correct", "wrong", "good", "great",
            "hello", "hey", "hi", "bye", "goodbye",
        }

        # Shared command router (Phase 3 of conversational flow refactor)
        self.router = ConversationRouter(
            skill_manager=skill_manager,
            conversation=conversation,
            llm=llm,
            reminder_manager=reminder_manager,
            memory_manager=memory_manager,
            news_manager=news_manager,
            context_window=context_window,
            conv_state=self.conv_state,
            config=config,
            web_researcher=self.web_researcher,
        )

    # ----- main loop -----

    def run(self):
        """Block on the event queue, dispatching events until shutdown."""
        self.logger.info("Coordinator event loop started")
        while self.running:
            try:
                event = self.event_queue.get(timeout=0.5)
            except queue.Empty:
                continue
            try:
                self._dispatch(event)
            except Exception as e:
                self.logger.error(f"Dispatch error for {event}: {e}", exc_info=True)
                self.stats['errors'] += 1
                self.stats['last_error_time'] = time.time()
                self.stats['last_error_msg'] = f"dispatch: {e}"

        self.logger.info("Coordinator event loop exited")

    def shutdown(self):
        self.running = False

    def get_health(self) -> dict:
        """Collect JARVIS internal state for health reporting."""
        # Listener state
        listener_health = {}
        if self.listener:
            listener_health = {
                'running': getattr(self.listener, 'running', False),
                'stream_active': getattr(self.listener, 'stream', None) is not None,
                'device': getattr(self.listener, 'device', 'unknown'),
                'conversation_active': getattr(self.listener, 'conversation_window_active', False),
                'rnnoise': getattr(self.listener, 'use_rnnoise', False),
            }

        # Skills count
        skills_loaded = 0
        intent_count = 0
        if self.skill_manager:
            skills_loaded = len(getattr(self.skill_manager, 'skills', {}))
            matcher = getattr(self.skill_manager, 'matcher', None)
            if matcher and hasattr(matcher, 'get_intent_count'):
                intent_count = matcher.get_intent_count()

        # LLM state
        llm_health = {}
        if self.llm:
            llm_health = {
                'api_call_count': getattr(self.llm, 'api_call_count', 0),
                'fallback_enabled': getattr(self.llm, 'fallback_enabled', False),
                'local_model': getattr(self.llm, 'local_model_path', None) is not None,
            }

        return {
            'running': self.running,
            'state': self.state.name,
            'stats': dict(self.stats),
            'event_queue_size': self.event_queue.qsize(),
            'tts_queue_size': self.tts_queue.qsize(),
            'managers': {
                'reminders': self.reminder_manager is not None,
                'news': self.news_manager is not None,
                'calendar': self.calendar_manager is not None,
                'profiles': self.profile_manager is not None,
                'memory': self.memory_manager is not None,
                'context_window': self.context_window is not None,
                'desktop': self.desktop_manager is not None,
            },
            'listener': listener_health,
            'tts_engine': getattr(self.tts, 'engine', 'unknown'),
            'llm': llm_health,
            'skills_loaded': skills_loaded,
            'semantic_intents': intent_count,
        }

    # ----- dispatch -----

    def _dispatch(self, event: Event):
        handlers = {
            EventType.TRANSCRIPTION_READY: self._handle_transcription,
            EventType.COMMAND_DETECTED: self._handle_command,
            EventType.PAUSE_LISTENING: lambda e: self.listener.pause_listening(),
            EventType.RESUME_LISTENING: self._handle_resume,
            EventType.OPEN_CONVERSATION_WINDOW: lambda e: self.listener.open_conversation_window(e.data),
            EventType.CLOSE_CONVERSATION_WINDOW: self._handle_close_conversation,
            EventType.SPEECH_STARTED: self._handle_speech_started,
            EventType.SPEECH_FINISHED: self._handle_speech_finished,
            EventType.LLM_COMPLETE: self._handle_llm_complete,
            EventType.SHUTDOWN: lambda e: self.shutdown(),
            EventType.ERROR: self._handle_error,
        }
        handler = handlers.get(event.type)
        if handler:
            handler(event)
        else:
            self.logger.debug(f"Unhandled event: {event.type.name}")

    # ----- transcription handling (extracted from _transcribe_and_check) -----

    def _handle_transcription(self, event: Event):
        """Process raw transcription text: validate, check wake word or
        conversation window, and emit COMMAND_DETECTED if appropriate."""
        # Extract text and speaker context from enriched or plain event data
        if isinstance(event.data, dict):
            raw_text = event.data["text"]
            speaker_id = event.data.get("speaker_id")
            speaker_confidence = event.data.get("speaker_confidence", 0.0)
            self._apply_speaker_context(speaker_id, speaker_confidence)
        else:
            raw_text = event.data

        text = raw_text.lower()

        # Filter noise annotations
        if (text.startswith('(') and text.endswith(')')) or \
           (text.startswith('[') and text.endswith(']')):
            self.logger.info(f"Ignoring noise annotation: {text}")
            print("‚ö†Ô∏è  Ignoring background noise")
            return

        # Filter garbage (repetitive chars)
        unique_chars = set(text.replace(' ', '').replace('.', ''))
        if len(unique_chars) <= 3 and len(text) > 5:
            self.logger.info(f"Ignoring garbage transcription: {text[:30]}...")
            return

        self.logger.info(f"Transcribed: {text}")
        print(f"üìù Heard: \"{text}\"")

        # Conversation window ‚Äî accept without wake word
        if self.listener.conversation_window_active:
            if self._is_conversation_noise(text):
                self.logger.info(f"Filtered noise during conversation: '{text}'")
                return
            text = self._apply_command_corrections(text)
            self.listener._cancel_conversation_timer()
            self.logger.info(f"Response during conversation window: {text}")
            self.event_queue.put(Event(
                EventType.COMMAND_DETECTED,
                data=text,
                source="coordinator",
            ))
            return

        # Wake word fuzzy match (threshold 0.80 ‚Äî eliminates "paris" 0.73, etc.)
        words = text.split()
        wake_word_found = False
        matched_word = ""
        for word in words:
            word_clean = word.strip('.,!?;:')
            similarity = SequenceMatcher(None, self.wake_word, word_clean).ratio()
            if similarity >= 0.80:
                self.logger.info(f"Wake word detected (similarity: {similarity:.2f}): {word_clean} in {text}")
                wake_word_found = True
                matched_word = word_clean
                break

        if wake_word_found:
            # Check if this is ambient conversation rather than a command
            if self._is_ambient_wake_word(text, matched_word):
                print("üîá Ambient mention (ignored)")
                return

            corrected_text = text.replace(matched_word, self.wake_word)
            self.logger.info(f"Corrected: '{text}' ‚Üí '{corrected_text}'")
            self.event_queue.put(Event(
                EventType.COMMAND_DETECTED,
                data=corrected_text,
                source="coordinator",
            ))
        else:
            self.logger.info(f"No wake word in: {text}")
            print("‚ùå No wake word (ignored)")

    # ----- command processing (extracted from on_command_detected) -----

    def _handle_command(self, event: Event):
        """Route a detected command through the priority chain."""
        full_text = event.data
        in_conversation = self.listener.conversation_window_active
        self.state = PipelineState.PROCESSING_COMMAND
        self.stats['commands_processed'] += 1

        # Parse input
        if in_conversation:
            self.logger.info(f"Conversation continues: {full_text}")
            print(f"\nüí¨ You said: {full_text}")
            if self.wake_word in full_text.lower():
                command = self._extract_command(full_text)
            else:
                command = full_text
        else:
            self.logger.info(f"Command detected: {full_text}")
            print(f"\nüü° Command detected: {full_text}")
            command = self._extract_command(full_text)
            # Fresh wake-word activation ‚Äî reset memory surfacing window
            # (covers conversation timeout path where no explicit close event fires)
            if self.memory_manager:
                self.memory_manager.reset_surfacing_window()

        # Pause listening while we process
        self.listener.pause_listening()

        # Beep only for fresh wake-word activation
        if not in_conversation and self.wake_word in full_text.lower():
            self._play_beep()

        if not command:
            self.logger.warning("No command extracted")
            self.listener.resume_listening()
            self.state = PipelineState.IDLE
            return

        self.logger.info(f"Command: {repr(command.strip())}")

        # --- Minimal greeting (voice-specific: adds "jarvis" to history) ---
        if command.strip() == "jarvis_only" or len(command.strip()) <= 2:
            self._handle_minimal_greeting(command, in_conversation)
            return

        # --- Process real command ---
        print(f"üìù Processing: {command}")
        self.logger.info(f"Processing command: {command}")
        self.conversation.add_message("user", command)
        self.tts._spoke = False

        # --- Route through shared priority chain ---
        result = self.router.route(command, in_conversation=in_conversation)

        # Skip: bare acknowledgment noise
        if result.skip:
            self.logger.info("Router: skip (bare ack noise)")
            self.listener.resume_listening()
            self.state = PipelineState.IDLE
            return

        if result.handled:
            response = result.text

            # Speak response (unless handler already spoke via TTS proxy,
            # e.g. deliver_rundown or a skill that calls tts.speak directly)
            if response and not self.tts._spoke:
                self._speak_and_wait(response)

            # Conversation window side effects
            if result.close_window:
                self._handle_close_conversation(None)
            elif result.open_window is not None:
                self.listener.open_conversation_window(result.open_window)
        else:
            # LLM fallback (streaming)
            print("ü§ñ Thinking...")
            response = self._stream_llm_response(
                result.llm_command, result.llm_history,
                memory_context=result.memory_context,
                conversation_messages=result.context_messages,
                raw_command=command,
            )
            if not response:
                response = "I'm sorry, I'm having trouble processing that right now."

        # Post-process: strip metric conversions Qwen sneaks in, then filler for history
        response = self.llm.strip_metric(response, command) if response else response
        stored_response = self.llm.strip_filler(response) if response else response
        self.conversation.add_message("assistant", stored_response)
        print(f"üí¨ Jarvis: {response}")
        if not self.tts._spoke:
            self._speak_and_wait(response)

        # Update centralized conversation state
        self.conv_state.update(
            command=command,
            response_text=response or "",
            response_type="llm" if not result.handled else "skill",
        )

        # Follow-up window ‚Äî handled results with explicit window instructions
        # skip the default window management
        if result.handled and (result.close_window or result.open_window is not None):
            pass  # Already handled above
        elif self.conversation.request_follow_up:
            duration = self.conversation.request_follow_up
            self.conversation.request_follow_up = None
            self.listener.open_conversation_window(duration)
        else:
            self._manage_conversation_window(response, in_conversation)

        # Stats and resume
        stats = self.conversation.get_conversation_stats()
        print(f"\nüìä Session: {stats['session_user_messages']} user, "
              f"{stats['session_assistant_messages']} assistant messages\n")
        self.listener.resume_listening()
        self.state = PipelineState.IDLE

    # ----- minimal greeting -----

    def _handle_minimal_greeting(self, command: str, in_conversation: bool):
        self.logger.info("Minimal greeting - just wake word")
        if self.reminder_manager and self.reminder_manager.has_rundown_mention():
            self.reminder_manager.clear_rundown_mention()
            response = persona.rundown_mention()
        else:
            response = persona.pick("greeting")

        self.conversation.add_message("user", "jarvis")
        if response:
            self.conversation.add_message("assistant", response)
            print(f"üí¨ Jarvis: {response}")
            self._speak_and_wait(response)

        self.listener.open_conversation_window(self.listener._extended_duration)
        self.listener.resume_listening()
        self.state = PipelineState.IDLE

    # ----- streaming LLM -----

    def _stream_llm_response(self, command: str, history: str,
                              memory_context: str = None,
                              conversation_messages: list = None,
                              raw_command: str = None) -> str:
        """Stream LLM response with first-chunk quality gating and tool calling.

        Streams tokens from Qwen, accumulates into sentence chunks,
        and speaks each chunk via a persistent aplay process for
        gapless multi-sentence playback.

        When tool calling is enabled, the LLM may request a web_search
        tool call instead of generating text. The pipeline will execute
        the search, feed results back, and stream the synthesized answer.

        Args:
            command: The (possibly augmented) text to send to the LLM.
            raw_command: The original user query before context augmentation.
                         Used for tool_choice regex and research exchange storage.
                         Falls back to command if not provided.
        """
        if raw_command is None:
            raw_command = command
        chunker = SpeechChunker()
        full_response = ""
        chunks_spoken = 0
        first_chunk_checked = False

        # Fire ack timer in case streaming is slow to start
        ack_timer = threading.Timer(0.3, self._play_ack_if_still_thinking)
        self._llm_responded = False
        ack_timer.daemon = True
        ack_timer.start()

        # Gapless audio pipeline (Kokoro only; Piper falls back to blocking)
        use_pipeline = (self.tts.engine == "kokoro")
        audio_pipeline = None

        # Choose tool-aware or plain streaming
        use_tools = self.llm.tool_calling and self.web_researcher

        try:
            pending_chunk = None
            tool_call_request = None

            # --- Phase A: stream from LLM (may yield ToolCallRequest) ---
            token_source = (
                self.llm.stream_with_tools(
                    user_message=command,
                    conversation_history=history,
                    memory_context=memory_context,
                    conversation_messages=conversation_messages,
                    raw_command=raw_command,
                ) if use_tools else
                self.llm.stream(
                    user_message=command,
                    conversation_history=history,
                    memory_context=memory_context,
                    conversation_messages=conversation_messages,
                )
            )

            for item in token_source:
                # Tool call sentinel ‚Äî break to Phase B
                if isinstance(item, ToolCallRequest):
                    tool_call_request = item
                    if not self._llm_responded:
                        self._llm_responded = True
                        ack_timer.cancel()
                    break

                # Regular token
                token = item
                if not self._llm_responded:
                    self._llm_responded = True
                    ack_timer.cancel()

                full_response += token
                chunk = chunker.feed(token)

                if chunk:
                    chunks_spoken, first_chunk_checked, pending_chunk, audio_pipeline = \
                        self._process_speech_chunk(
                            chunk, command, history, memory_context,
                            conversation_messages, chunks_spoken,
                            first_chunk_checked, pending_chunk,
                            audio_pipeline, use_pipeline,
                        )
                    if chunks_spoken == -1:  # quality gate failed
                        return pending_chunk  # contains fallback response

            # --- Phase B: handle tool call if requested ---
            if tool_call_request:
                self.logger.info(
                    f"üîç Web search requested: {tool_call_request.arguments}"
                )
                print(f"üîç Searching: {tool_call_request.arguments.get('query', '')}")

                # No interim ack here ‚Äî the 0.3s ack timer already fires
                # one of the curated phrases before the tool call arrives.

                # Execute the search
                if tool_call_request.name == "web_search":
                    query = tool_call_request.arguments.get("query", command)
                    results = self.web_researcher.search(query)
                    self.conv_state.research_results = results

                    # Fetch page content from top 3 results concurrently.
                    # Multiple sources let the LLM cross-reference and pick the best info.
                    page_sections = self.web_researcher.fetch_pages_parallel(results)

                    page_content = ""
                    if page_sections:
                        page_content = "\n\nFull article content:\n\n" + \
                            "\n\n---\n\n".join(page_sections)

                    tool_result = format_search_results(results) + page_content
                    print(f"üìã Found {len(results)} results")
                else:
                    tool_result = f"Unknown tool: {tool_call_request.name}"

                # Stream synthesized answer from tool results
                for token in self.llm.continue_after_tool_call(
                    tool_call_request, tool_result
                ):
                    if not self._llm_responded:
                        self._llm_responded = True

                    full_response += token
                    chunk = chunker.feed(token)

                    if chunk:
                        chunks_spoken, first_chunk_checked, pending_chunk, audio_pipeline = \
                            self._process_speech_chunk(
                                chunk, command, history, memory_context,
                                conversation_messages, chunks_spoken,
                                first_chunk_checked, pending_chunk,
                                audio_pipeline, use_pipeline,
                            )
                        if chunks_spoken == -1:
                            return pending_chunk

            # Combine buffered last chunk + flush remnant, strip filler, then speak
            remaining = chunker.flush()
            final_text = (pending_chunk or "") + (" " + remaining if remaining else "")

            if final_text.strip():
                if not first_chunk_checked:
                    quality_issue = self.llm._check_response_quality(final_text, command)
                    if quality_issue:
                        self.logger.warning(
                            f"Streaming quality gate failed ({quality_issue}): "
                            f"'{final_text[:60]}' ‚Äî falling back to sync chat()"
                        )
                        if audio_pipeline:
                            audio_pipeline.finish()
                        return self.llm.chat(
                            user_message=command,
                            conversation_history=history,
                            memory_context=memory_context,
                            conversation_messages=conversation_messages,
                        )
                final_text = self.llm.strip_filler(self.llm.strip_metric(final_text, command))
                if final_text.strip():
                    if audio_pipeline:
                        audio_pipeline.put(final_text)
                    else:
                        self._speak_and_wait(final_text)
                    chunks_spoken += 1

            # Wait for all audio to finish playing
            if audio_pipeline:
                audio_pipeline.finish()

        except Exception as e:
            self.logger.error(f"Streaming LLM error: {e}")
            self._llm_responded = True
            ack_timer.cancel()
            if audio_pipeline:
                audio_pipeline.finish()
            if not full_response:
                return self.llm.chat(
                    user_message=command,
                    conversation_history=history,
                    memory_context=memory_context,
                    conversation_messages=conversation_messages,
                )

        # Cancel ack timer if stream was empty
        if not self._llm_responded:
            self._llm_responded = True
            ack_timer.cancel()

        if chunks_spoken > 0:
            self.logger.info(f"Streamed LLM response in {chunks_spoken} chunks")
            self.tts._spoke = True

        # Extended conversation window after research answers
        if tool_call_request:
            self.conversation.request_follow_up = 15.0
            # Store the exchange so follow-ups have context.
            # Use raw_command (not the augmented command) to prevent nested
            # context wrapping on successive follow-ups.
            self.conv_state.set_research_context(
                results=self.conv_state.research_results or [],
                exchange={"query": raw_command, "answer": full_response},
            )

        return full_response

    def _process_speech_chunk(self, chunk, command, history, memory_context,
                              conversation_messages, chunks_spoken,
                              first_chunk_checked, pending_chunk,
                              audio_pipeline, use_pipeline):
        """Process a completed sentence chunk for speech.

        Returns updated (chunks_spoken, first_chunk_checked, pending_chunk, audio_pipeline).
        On quality gate failure, returns (-1, ..., fallback_response, ...).
        """
        if not first_chunk_checked:
            first_chunk_checked = True
            quality_issue = self.llm._check_response_quality(chunk, command)
            if quality_issue:
                self.logger.warning(
                    f"Streaming quality gate failed ({quality_issue}): "
                    f"'{chunk[:60]}' ‚Äî falling back to sync chat()"
                )
                fallback = self.llm.chat(
                    user_message=command,
                    conversation_history=history,
                    memory_context=memory_context,
                    conversation_messages=conversation_messages,
                )
                return -1, first_chunk_checked, fallback, audio_pipeline

        if chunks_spoken == 0 and pending_chunk is None:
            # First chunk ‚Äî strip redundant opener if ack already played
            if self.tts.ack_played:
                chunk = self._strip_ack_opener(chunk)
                self.tts.clear_ack_played()
                if not chunk:
                    return chunks_spoken, first_chunk_checked, pending_chunk, audio_pipeline
            processed = self.llm.strip_metric(chunk, command)
            self.listener.speaking = True
            if use_pipeline:
                audio_pipeline = StreamingAudioPipeline(
                    self.tts, self.logger
                )
                audio_pipeline.start()
                audio_pipeline.put(processed)
            else:
                self._speak_and_wait(processed)
            chunks_spoken += 1
        else:
            # Buffer subsequent chunks; submit the previous one
            if pending_chunk:
                processed = self.llm.strip_metric(pending_chunk, command)
                if audio_pipeline:
                    audio_pipeline.put(processed)
                else:
                    self._speak_and_wait(processed)
                chunks_spoken += 1
            pending_chunk = chunk

        return chunks_spoken, first_chunk_checked, pending_chunk, audio_pipeline

    # ----- TTS helpers -----

    def _speak_and_wait(self, text: str):
        """Speak text synchronously (blocks until playback finishes).

        For the coordinator thread this is fine ‚Äî we don't need to process
        other events while speaking a response to the current command.
        """
        self.listener.speaking = True
        self.tts.speak(text)

    # Regex to strip redundant LLM opening phrases when ack already played
    _ACK_OPENER_RE = re.compile(
        r'^(Certainly|Of course|Very well|Right away|One moment|Just a moment|'
        r'Give me (?:just )?a moment|Absolutely|Sure thing|One second)'
        r'[,.]?\s*(?:sir|ma\'am|miss)?\.?\s*',
        re.IGNORECASE,
    )

    def _play_ack_if_still_thinking(self):
        """Timer callback ‚Äî plays ack if LLM hasn't responded yet."""
        if not self._llm_responded:
            self.tts.speak_ack()

    def _strip_ack_opener(self, text: str) -> str:
        """Strip leading ack phrase from LLM text if ack was already spoken."""
        stripped = self._ACK_OPENER_RE.sub('', text)
        if stripped != text:
            # Capitalize the new leading character
            stripped = stripped.lstrip()
            if stripped:
                stripped = stripped[0].upper() + stripped[1:]
            self.logger.info(f"Stripped ack opener: '{text[:40]}' ‚Üí '{stripped[:40]}'")
        return stripped

    def _play_beep(self):
        """Play wake-word acknowledgment beep."""
        try:
            if not self.beep_path.exists():
                return
            import subprocess
            audio_device = self.config.get("audio.output_device", "plughw:0,0")
            subprocess.run(
                ["aplay", "-D", audio_device, str(self.beep_path)],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=2,
            )
        except Exception as e:
            self.logger.error(f"Failed to play beep: {e}")

    # ----- conversation helpers -----

    def _handle_close_conversation(self, event: Event):
        """Handle explicit conversation window close + reset memory surfacing."""
        self.listener.close_conversation_window()
        if self.memory_manager:
            self.memory_manager.reset_surfacing_window()
        if self.context_window:
            self.context_window.reset()
        # Reset centralized conversation state (clears research cache,
        # jarvis_asked_question, last intent/response tracking)
        self.conv_state.close_window()
        if self.web_researcher:
            self.web_researcher.clear_cache()

    def _manage_conversation_window(self, response: str, was_in_conversation: bool):
        """Decide whether to open/extend the conversation window."""
        if was_in_conversation:
            if self.conversation.should_open_follow_up_window(response):
                duration = self.listener._extended_duration
            else:
                duration = self.listener._default_duration
            self.listener.open_conversation_window(duration)
        elif self.conversation.should_open_follow_up_window(response):
            self.listener.open_conversation_window(self.listener._extended_duration)
        else:
            self.listener.open_conversation_window(self.listener._default_duration)

    def _extract_command(self, full_text: str) -> str:
        """Extract command text from transcription (remove wake word)."""
        import string
        text_lower = full_text.lower()
        wake_pos = text_lower.find(self.wake_word)

        if wake_pos == -1:
            return full_text.strip()

        before = full_text[:wake_pos].strip()
        after = full_text[wake_pos + len(self.wake_word):].strip()

        before = before.strip(string.punctuation + ' ')
        after = after.strip(string.punctuation + ' ')

        if after:
            return after
        elif before:
            return before
        else:
            return "jarvis_only"

    # ----- noise / correction helpers (from continuous_listener) -----

    def _is_conversation_noise(self, text: str) -> bool:
        """Check if text during conversation window is likely noise."""
        if len(text) < 2:
            return True
        unique_chars = set(text.replace(' ', ''))
        if len(unique_chars) <= 3 and len(text) > 5:
            return True
        words = text.strip().split()
        if len(words) == 1 and words[0] not in self._valid_short_replies:
            if len(words[0]) < 4:
                return True
        return False

    def _apply_command_corrections(self, text: str) -> str:
        """Apply corrections for common command mishearings."""
        import re
        if re.match(r'^i (was|analyzed)\s+', text, re.IGNORECASE):
            return re.sub(r'^i (was|analyzed)\s+', 'analyze ', text, flags=re.IGNORECASE)
        if text.lower().startswith("i'm "):
            return "analyze " + text[4:]
        return text

    # ----- ambient wake word filter -----

    # Words that follow "jarvis" in ambient speech (talking ABOUT jarvis)
    # but never follow "jarvis," in a command.
    _AMBIENT_FOLLOWERS = frozenset({
        'is', 'was', 'has', 'had', 'will', 'would', 'can', 'could',
        'does', 'did', 'should', 'might', 'may', 'of',
    })

    # Prefixes that legitimately precede the wake word (e.g. "hey jarvis")
    _WAKE_PREFIXES = frozenset({
        'hey', 'hi', 'yo', 'morning', 'good', 'okay', 'ok',
    })

    def _is_ambient_wake_word(self, text: str, matched_word: str) -> bool:
        """Determine if a wake word detection is ambient conversation, not a command.

        Uses position, post-wake-word analysis, and utterance length to
        distinguish "Jarvis, what time is it?" from "he was talking about Jarvis".

        Returns True if the wake word should be IGNORED (ambient).
        """
        words = text.split()

        # Find word index of the matched wake word
        word_idx = None
        for i, w in enumerate(words):
            if w.strip('.,!?;:\'"') == matched_word:
                word_idx = i
                break

        if word_idx is None:
            return False  # Can't determine ‚Äî let it through

        # --- Signal 1: Position ---
        # Real commands have "jarvis" in the first 2 words (or 3 with a prefix
        # like "hey" / "good morning").
        effective_pos = word_idx
        if word_idx <= 2:
            # Check if earlier words are known prefixes
            prefix_words = [w.strip('.,!?;:') for w in words[:word_idx]]
            if all(pw in self._WAKE_PREFIXES for pw in prefix_words):
                effective_pos = 0  # Treat as position 0

        if effective_pos >= 3:
            self.logger.info(
                f"üîá Ambient rejected (position {word_idx}): {text[:80]}"
            )
            return True

        # --- Signal 2: Post-wake-word copula/auxiliary ---
        # "jarvis is listening" = ambient.  "jarvis, is it raining?" = command.
        # The comma after "jarvis" is the key differentiator.
        if word_idx < len(words):
            wake_token = words[word_idx]  # e.g. "jarvis," or "jarvis" or "jarvis's"

            # Possessive = always ambient ("jarvis's brain")
            if wake_token.endswith("'s") or wake_token.endswith("\u2019s"):
                self.logger.info(
                    f"üîá Ambient rejected (possessive): {text[:80]}"
                )
                return True

            has_comma = wake_token.endswith(',')
            if not has_comma and word_idx + 1 < len(words):
                next_word = words[word_idx + 1].strip('.,!?;:').lower()
                if next_word in self._AMBIENT_FOLLOWERS:
                    self.logger.info(
                        f"üîá Ambient rejected ('{matched_word} {next_word}' "
                        f"without comma): {text[:80]}"
                    )
                    return True

        # --- Signal 5: Length heuristic ---
        # Very long utterances where wake word isn't the opener are
        # almost certainly ambient conversation, not commands.
        if len(words) > 15 and word_idx > 0:
            self.logger.info(
                f"üîá Ambient rejected (long utterance {len(words)} words, "
                f"wake word at position {word_idx}): {text[:80]}"
            )
            return True

        return False

    # ----- speaker context -----

    def _apply_speaker_context(self, speaker_id: Optional[str], confidence: float):
        """Set honorific and conversation user based on speaker identification."""
        if speaker_id and self.profile_manager:
            honorific = self.profile_manager.get_honorific_for(speaker_id)
            set_honorific(honorific)
            self.conversation.current_user = speaker_id
            self.logger.info(
                f"Speaker identified: {speaker_id} (confidence={confidence:.3f}, "
                f"honorific={honorific})"
            )
        elif speaker_id is None and self.profile_manager:
            # Unknown speaker ‚Äî keep current honorific (default "sir")
            self.logger.debug(f"Speaker unknown (confidence={confidence:.3f})")

    # ----- resume handler -----

    def _handle_resume(self, event: Event):
        """Handle RESUME_LISTENING ‚Äî suppressed while streaming is active."""
        if self._streaming_active:
            self.logger.debug("Suppressing resume ‚Äî streaming active")
            return
        self.listener.resume_listening()

    # ----- speech lifecycle -----

    def _handle_speech_started(self, event: Event):
        self.logger.debug("Speech started")

    def _handle_speech_finished(self, event: Event):
        self.logger.debug("Speech finished")

    # ----- LLM complete -----

    def _handle_llm_complete(self, event: Event):
        """Handle LLM streaming completion."""
        self._streaming_active = False
        self.logger.info("LLM streaming complete")

    # ----- error handling -----

    def _handle_error(self, event: Event):
        data = event.data or {}
        source = data.get("source", "unknown")
        error = data.get("error", "unknown error")
        self.logger.error(f"Pipeline error from {source}: {error}")

        self.stats['errors'] += 1
        self.stats['last_error_time'] = time.time()
        self.stats['last_error_msg'] = f"{source}: {error}"

        if source == "tts":
            # TTS failure ‚Äî ensure listening resumes
            self.listener.resume_listening()
