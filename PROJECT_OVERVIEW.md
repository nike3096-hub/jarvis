# JARVIS - Personal AI Assistant

**Version:** 2.5.0 (Production Ready)
**Last Updated:** February 21, 2026
**Status:** âœ… Stable, Feature-Rich, Voice-Controlled

---

## ğŸ“‹ Table of Contents
- [What is JARVIS?](#what-is-jarvis)
- [Current Capabilities](#current-capabilities)
- [System Architecture](#system-architecture)
- [Technology Stack](#technology-stack)
- [Progress Timeline](#progress-timeline)
- [Design Philosophy](#design-philosophy)
- [Roadmap](#roadmap)
- [Getting Started](#getting-started)

---

## ğŸ¤– What is JARVIS?

JARVIS (Just A Rather Very Intelligent System) is a fully offline, voice-controlled AI assistant. Unlike commercial assistants, JARVIS runs entirely on your local hardware with:

- âœ… **Complete Privacy** - No cloud, no data collection
- âœ… **Custom Voice Training** - Learns YOUR accent
- âœ… **Modular Skills** - Easy to extend
- âœ… **Natural Conversation** - Semantic understanding
- âœ… **Production Ready** - Stable, tested, reliable

**Hardware:** Runs on consumer-grade PC (Ryzen 9 5900X, AMD RX 7900 XT)  
**OS:** Ubuntu 24.04 LTS  
**Latency:** 300-600ms for skill queries, 2-4s for LLM fallback (streaming)

---

## ğŸ¯ Current Capabilities

### Core Features
- **Wake Word Detection** - Porcupine "Jarvis" with 100% accuracy
- **Speech Recognition** - Fine-tuned Whisper v2 (CTranslate2, GPU-accelerated, 94%+ accuracy, 198 phrases, Southern accent)
- **Natural Language Understanding** - Semantic intent matching (sentence-transformers)
- **Conversational Flow Engine** - Persona module (10 response pools, ~50 templates), ConversationState (turn tracking), ConversationRouter (shared priority chain)
- **Text-to-Speech** - Kokoro 82M (primary, CPU, fable+george blend) + Piper ONNX fallback
- **LLM Intelligence** - Qwen 3-8B (Q5_K_M) via llama.cpp + Claude API fallback with quality gating
- **Web Research** - Qwen 3-8B native tool calling + DuckDuckGo + trafilatura, multi-source synthesis
- **Event-Driven Pipeline** - Coordinator with STT/TTS workers, streaming LLM, contextual ack cache (10 tagged phrases)
- **Gapless TTS Streaming** - StreamingAudioPipeline with single persistent aplay, background Kokoro generation
- **Adaptive Conversation Windows** - 4-7s duration, extends with conversation depth, timeout cleanup, noise filtering, dismissal detection
- **Ambient Wake Word Filter** - Multi-signal: position, copula, threshold 0.80, length â€” blocks ambient mentions
- **Three Frontends** - Voice (production), console (debug/hybrid), web UI (browser-based chat with streaming + sessions)
- **Web UI** - aiohttp WebSocket server, streaming LLM, markdown rendering, session sidebar, health HUD, file handling

### Skills (11 Active)

#### ğŸŒ¤ï¸ Weather
- Current conditions, forecasts, rain probability
- *"Jarvis, what's the weather like?"*

#### â° Time & Date
- Current time, date, day of week
- *"Jarvis, what time is it?"*

#### ğŸ’» System Information
- CPU, memory, disk, uptime, network
- *"Jarvis, what's my CPU usage?"*

#### ğŸ—‚ï¸ Filesystem
- File search, code line counting, script analysis
- *"Jarvis, how many lines of code in your codebase?"*

#### ğŸ“ File Editor
- Write, edit, read, delete files + list share contents
- LLM-generated content, confirmation flow for destructive operations
- *"Jarvis, write a backup script"*
- *"Jarvis, delete temp.txt"*

#### ğŸ› ï¸ Developer Tools
- 13 intents: codebase search, git multi-repo, system admin, general shell
- "Show me" visual output, 3-tier safety (allowlist â†’ confirmation â†’ blocked)
- *"Jarvis, show me the git status"*

#### ğŸŒ Web Navigation
- Playwright-based search, result selection, page navigation
- Scroll pagination (YouTube/Reddit), window management
- *"Jarvis, search for Python async tutorials"*

#### ğŸ“° News
- 16 RSS feeds, urgency classification, semantic dedup
- Voice headline delivery, category filtering
- *"Jarvis, read me the tech headlines"*

#### ğŸ”” Reminders
- Priority tones, nag behavior, acknowledgment tracking
- Google Calendar two-way sync, dedicated JARVIS calendar
- Daily & weekly rundowns (state machine: offered â†’ re-asked â†’ deferred â†’ retry)
- *"Jarvis, remind me to call the dentist at 3pm"*

#### ğŸ–¥ï¸ Desktop Control (App Launcher)
- 16 intents: launch/close apps, fullscreen/minimize/maximize, volume up/down/mute, workspace switch/move, focus app, list windows, clipboard read/write
- GNOME Shell extension D-Bus bridge for Wayland-native window management
- *"Jarvis, open Chrome"*
- *"Jarvis, volume up"*
- *"Jarvis, switch to workspace 2"*

#### ğŸ’¬ Conversation
- Greetings, small talk, acknowledgments, butler personality
- *"Jarvis, how are you?"*

### Additional Systems
- **Conversational Memory** - SQLite fact store + FAISS semantic search, recall, batch LLM extraction, proactive surfacing, forget/transparency
- **Context Window** - Topic-segmented working memory, relevance-scored assembly, cross-session persistence
- **User Profiles** - Speaker identification (resemblyzer d-vectors), dynamic honorifics, voice enrollment
- **Google Calendar** - OAuth, event CRUD, incremental sync, background polling
- **Cross-Session Memory** - Last 32 messages loaded from persistent history
- **Health Check** - 5-layer system diagnostic (ANSI terminal report + voice summary)
- **Hardware Failure Handling** - Startup retry, device monitoring, degraded mode, graceful recovery
- **GNOME Desktop Bridge** - Custom GNOME Shell extension (D-Bus), Wayland-native window management, wmctrl fallback
- **GitHub Publishing** - Automated redaction pipeline, PII verification, non-interactive `--auto` publish

---

## ğŸ—ï¸ System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       USER VOICE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AUDIO INPUT (FIFINE K669B USB Mic - Mono 48kHz)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VAD (Voice Activity Detection) - WebRTC VAD             â”‚
â”‚  â€¢ Detects speech vs silence                            â”‚
â”‚  â€¢ Triggers transcription                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STT (Speech-to-Text) - Custom Whisper Model v2          â”‚
â”‚  â€¢ Fine-tuned on user's Southern accent                 â”‚
â”‚  â€¢ 198 training phrases, 94%+ accuracy                  â”‚
â”‚  â€¢ GPU-accelerated: 0.1-0.2s transcription              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WAKE WORD DETECTION + AMBIENT FILTER                   â”‚
â”‚  â€¢ Checks for "Jarvis" in transcript                    â”‚
â”‚  â€¢ Fuzzy matching (threshold: 0.80)                     â”‚
â”‚  â€¢ Ambient filter: position, copula, length             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONVERSATION ROUTER - 7-Layer Priority Chain           â”‚
â”‚  Layer 1: Confirmation interception                     â”‚
â”‚  Layer 2: Dismissal / conversation close                â”‚
â”‚  Layer 3: Memory / context / news pull-up               â”‚
â”‚  Layer 4: Exact match (time, date)                      â”‚
â”‚  Layer 5: Keyword + semantic verify                     â”‚
â”‚  Layer 6: Pure semantic matching                        â”‚
â”‚  Layer 7: LLM fallback                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SKILL EXECUTION                                         â”‚
â”‚  â€¢ Modular skill system                                 â”‚
â”‚  â€¢ Semantic intent handlers                             â”‚
â”‚  â€¢ Error handling & logging                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM - Qwen 3-8B via REST API + Claude API fallback    â”‚
â”‚  â€¢ Handles unmatched queries                            â”‚
â”‚  â€¢ Web research via native tool calling                 â”‚
â”‚  â€¢ Conversational responses + technical reasoning       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TTS (Text-to-Speech) - Kokoro 82M + Piper fallback     â”‚
â”‚  â€¢ 50/50 fable+george voice blend                       â”‚
â”‚  â€¢ Natural intonation, streaming output                 â”‚
â”‚  â€¢ CPU-only, low latency                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AUDIO OUTPUT                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Example
```
User: "Jarvis, how many lines of code in your codebase?"
  â†“ (Audio captured)
VAD: Speech detected
  â†“ (Transcription triggered)
Whisper: "jarvis, how many lines of code in your codebase?"
  â†“ (Wake word check)
Wake Word: âœ… Detected "jarvis" (similarity: 1.00)
  â†“ (Strip wake word)
Intent Matching: "how many lines of code in your codebase"
  â†“ (Semantic match)
Semantic Matcher: 0.95 score â†’ FilesystemSkill.count_code_lines
  â†“ (Execute handler)
Filesystem Skill: Count Python files, exclude venv
  â†“ (Return response)
Response: "My codebase contains 320,388 lines of Python code across 40 files, sir."
  â†“ (TTS)
Kokoro: Generates audio
  â†“ (Playback)
User: Hears response
```

---

## ğŸ› ï¸ Technology Stack

### Core Components
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **STT** | faster-whisper (CTranslate2, fine-tuned) | Speech recognition |
| **TTS** | Kokoro 82M (primary) + Piper (fallback) | Speech synthesis |
| **LLM** | Qwen 3-8B (Q5_K_M via llama.cpp) + Claude API | Language understanding + web research |
| **VAD** | WebRTC VAD | Voice activity detection |
| **Wake Word** | Porcupine | Trigger detection |
| **Embeddings** | sentence-transformers | Intent matching |

### Infrastructure
- **OS:** Ubuntu 24.04 LTS
- **Python:** 3.12
- **Service Manager:** systemd (user services)
- **LLM Server:** llama-server (REST API)
- **Storage:**
  - Code: `~/jarvis/`
  - Skills: `/mnt/storage/jarvis/skills/`
  - Models: `/mnt/models/` (4TB dedicated drive)
  - Public repo: `~/jarvis-public/` â†’ `github.com/InterGenJLU/jarvis`

### Key Libraries
- `torch` (ROCm) - PyTorch for ML models (CPU-only for TTS)
- `ctranslate2` - GPU-accelerated Whisper inference
- `sentence-transformers` - Semantic intent matching
- `kokoro` - Primary TTS engine (82M model)
- `sounddevice` - Audio I/O
- `playwright` - Headless web navigation
- `faiss-cpu` - Vector search for conversational memory
- `numpy` - Array operations
- `requests` - HTTP client for LLM API
- `pyyaml` - Configuration

---

## ğŸ“ˆ Progress Timeline

### Phase 1: Foundation (Days 1-3)
- âœ… Basic voice loop (wake word â†’ command â†’ response)
- âœ… Whisper integration (base model)
- âœ… Piper TTS setup
- âœ… Simple command patterns

### Phase 2: Skills System (Days 4-7)
- âœ… Modular skill architecture
- âœ… Weather skill (OpenWeatherMap API)
- âœ… Time/date skill
- âœ… System info skill
- âœ… Conversation skill

### Phase 3: Intelligence (Days 8-10)
- âœ… Semantic intent matching (90% pattern reduction)
- âœ… LLM integration (Mistral 7B â†’ Qwen 2.5-7B â†’ Qwen 3-8B)
- âœ… Conversation context window
- âœ… Intent confidence scoring

### Phase 4: Production Ready (Feb 9-10)
- âœ… Git version control (3 repositories)
- âœ… Automated backups (daily, systemd)
- âœ… 4TB model storage setup
- âœ… Paul Bettany voice cloning (proof-of-concept)
- âœ… Comprehensive documentation

### Phase 5: Major Upgrades (Feb 11) ğŸš€
- âœ… **Qwen 3-8B LLM** (better reasoning)
- âœ… **Custom Whisper training** (88%+ accuracy)
- âœ… **Filesystem skill** (semantic file operations)
- âœ… **Audio optimization** (no overflow)
- âœ… **Skill development guide** (comprehensive docs)

### Phase 6: GPU + CTranslate2 (Feb 12-13) ğŸš€
- âœ… **GPU-Accelerated STT** â€” CTranslate2 with ROCm on RX 7900 XT (0.1-0.2s)
- âœ… **PyTorch + CTranslate2 coexistence** â€” torch 2.10.0+rocm7.1 + CT2 4.7.1
- âœ… **Three-repo architecture** â€” code, skills, models on separate drives

### Phase 7: Feature Explosion (Feb 14-17) ğŸš€
- âœ… **12 critical bug fixes** â€” Whisper pre-buffer, semantic routing, keyword greediness, VAD overlap, etc.
- âœ… **News headlines** â€” 16 RSS feeds, urgency classification, semantic dedup, voice delivery
- âœ… **Reminder system** â€” priority tones, nag behavior, ack tracking, Google Calendar 2-way sync
- âœ… **Web Navigation Phase 2** â€” result selection, page nav, scroll pagination, window management
- âœ… **Developer tools skill** â€” 13 intents, codebase search, git multi-repo, system admin, safety tiers
- âœ… **Console mode** â€” text/hybrid/speech modes with stats panel
- âœ… **FIFINE K669B mic upgrade** â€” udev rule, config updated

### Phase 8: Polish + Advanced Systems (Feb 15-17) ğŸš€
- âœ… **Kokoro TTS** â€” 82M model, 50/50 fable+george blend, Piper fallback
- âœ… **Latency refactor (4 phases)** â€” streaming TTS, ack cache, streaming LLM, event pipeline
- âœ… **User profile system (5 phases)** â€” honorific, ProfileManager, SpeakerIdentifier, pipeline, enrollment
- âœ… **Honorific refactoring** â€” ~470 "sir" instances â†’ dynamic `{honorific}` across 19 files
- âœ… **Conversational memory (6 phases)** â€” SQLite facts, FAISS indexing, recall, batch extraction, proactive surfacing, forget/transparency
- âœ… **Context window (4 phases)** â€” topic-segmented working memory, relevance-scored assembly, cross-session persistence
- âœ… **System health check** â€” 5-layer diagnostic, ANSI terminal + voice summary
- âœ… **Gapless TTS streaming** â€” StreamingAudioPipeline, single persistent aplay, zero-gap playback
- âœ… **Hardware failure graceful degradation** â€” startup retry, device monitor, degraded mode

### Phase 9: Web Research + Hardening (Feb 17-18) ğŸš€
- âœ… **Web research (5 phases)** â€” Qwen 3-8B native tool calling + DuckDuckGo + trafilatura, multi-source synthesis
- âœ… **Prescriptive prompt design** â€” explicit rules for Qwen tool-use decisions, 150/150 correct test decisions
- âœ… **Streaming delivery fixes** â€” sentence-only chunking, per-chunk metric stripping, context flush on shutdown
- âœ… **27 bug fixes** â€” ack collision, keyword greediness, dismissal detection, decimal TTS, aplay lazy open, chunker decimal split, and more
- âœ… **Scoped TTS subprocess control** â€” replaced global `pkill -9` with tracked subprocess kill
- âœ… **GitHub publishing system** â€” automated redaction, PII verification, public repo sync

### Phase 10: Desktop Integration + Tooling (Feb 19-20) ğŸš€
- âœ… **GNOME Desktop Integration (5 phases)** â€” Custom GNOME Shell extension with D-Bus bridge, 14 D-Bus methods
- âœ… **Desktop Manager** â€” Singleton module with lazy D-Bus, wmctrl fallback, pactl, notify-send, wl-clipboard
- âœ… **App Launcher Skill v2.0** â€” 16 intents: launch/close, fullscreen/minimize/maximize, volume, workspace, focus, clipboard
- âœ… **Desktop notifications** â€” Wired into reminder system via notify-send
- âœ… **Publish script non-interactive mode** â€” `--auto` flag for CI-friendly publish (auto-generate commit msg + push)

### Phase 11: Web Chat UI (Feb 20) ğŸš€
- âœ… **5-phase implementation** â€” aiohttp WebSocket server, vanilla HTML/CSS/JS, zero new dependencies
- âœ… **Streaming LLM** â€” Token-by-token delivery with quality gate (buffers first sentence, retries if gibberish)
- âœ… **File handling** â€” Drag/drop, /file, /clipboard, /append, /context slash commands
- âœ… **History + notifications** â€” Paginated `/api/history`, scroll-to-load-more, floating announcement banners
- âœ… **Polish** â€” Markdown rendering with XSS protection, code blocks + copy, responsive breakpoints
- âœ… **Session sidebar** â€” 30-min gap detection, hamburger toggle, session rename, pagination, LIVE badge

### Phase 12: File Editor + Edge Case Testing (Feb 20) ğŸš€
- âœ… **File Editor Skill** â€” 5 intents (write, edit, read, delete, list share), confirmation flow, LLM-generated content
- âœ… **Ambient Wake Word Filter** â€” Multi-signal: position, copula, threshold 0.80, length â€” blocks ambient mentions
- âœ… **Edge Case Testing Phase 1** â€” ~200 test cases across 9 phases, 37/40 pass (92.5%), 14 routing failures fixed

### Phase 13: Conversational Flow Refactor (Feb 21) ğŸš€
- âœ… **Phase 1: Persona** â€” 10 response pools (~50 templates), system prompts, honorific injection
- âœ… **Phase 2: ConversationState** â€” Turn counting, intent history, question detection, research context
- âœ… **Phase 3: ConversationRouter** â€” Shared priority chain for voice/console/web (one router, three frontends)
- âœ… **Phase 4: Response Flow Polish** â€” Contextual ack selection (10 tagged phrases), smarter follow-up windows, timeout cleanup, suppress LLM opener collision
- âœ… **38 router tests** â€” `scripts/test_router.py` validates routing decisions without live LLM/mic

### Phase 14: Whisper v2 Fine-Tuning (Feb 21) ğŸš€
- âœ… **198 training phrases** (up from 149), FIFINE K669B USB condenser mic
- âœ… **GPU fp16 training** â€” 89 seconds on RX 7900 XT
- âœ… **94.4% live accuracy** â€” wake word 100%, contraction handling 100%

---

## ğŸ¨ Design Philosophy

### 1. Privacy First
All processing happens locally. No data leaves your machine. No telemetry, no cloud dependencies.

### 2. Modular & Extensible
Skills are independent modules. Add new capabilities without touching core code.

### 3. Natural Interaction
Semantic matching allows flexible phrasing. Say it naturally, JARVIS understands.

### 4. Production Quality
- Comprehensive error handling
- Extensive logging
- Graceful degradation
- Auto-recovery mechanisms

### 5. Hardware Efficient
Optimized for consumer hardware. No expensive GPUs required (though AMD GPU supported).

### 6. Maintainable
- Clean code structure
- Comprehensive documentation
- Version controlled
- Automated backups

---

## ğŸ—ºï¸ Roadmap

### Recently Completed
- [x] ~~Whisper v2 retraining~~ â€” Done (Feb 21). 198 phrases, 94%+ accuracy
- [x] ~~Conversational Flow Refactor (4 phases)~~ â€” Done (Feb 21). Persona, State, Router, Polish
- [x] ~~Web Chat UI (5 phases)~~ â€” Done (Feb 20). Streaming, sessions, markdown
- [x] ~~File Editor Skill~~ â€” Done (Feb 20). 5 intents, confirmation flow
- [x] ~~Edge Case Testing Phase 1~~ â€” Done (Feb 20). 92.5% pass rate
- [x] ~~Ambient Wake Word Filter~~ â€” Done (Feb 20). Multi-signal blocking
- [x] ~~App launcher + desktop control~~ â€” Done (Feb 19). 16 intents, GNOME Shell extension
- [x] ~~Web research (Qwen tool calling)~~ â€” Done (Feb 18). DuckDuckGo + trafilatura
- [x] ~~GitHub open source publication~~ â€” Done (Feb 18). Automated PII redaction

### Up Next
- [ ] Edge Case Testing Phase 2 (priority chain & state machines)
- [ ] Document generation skill
- [ ] Email skill (Gmail)
- [ ] Google Keep integration

### Medium Term
- [ ] Audio recording skill
- [ ] LLM-centric architecture migration (wait for Qwen 3.5)
- [ ] Music control (Apple Music)

### Long Term
- [ ] Threat hunting / malware analysis framework
- [ ] Video / face recognition
- [ ] Home automation
- [ ] Mobile access
- [ ] Emotional context awareness

---

## ğŸš€ Getting Started

### Prerequisites
- Ubuntu 24.04 LTS (or similar Linux)
- Python 3.11+
- 16GB+ RAM recommended
- GPU optional (AMD/NVIDIA for acceleration)

### Installation
```bash
# Clone repository
git clone <repo-url> ~/jarvis
cd ~/jarvis

# Install dependencies
pip install -r requirements.txt --break-system-packages

# Set up models directory
sudo mkdir -p /mnt/models
sudo chown $USER:$USER /mnt/models

# Download models (automated script coming soon)
# For now, manually place models in /mnt/models/

# Configure
cp config.yaml.example config.yaml
# Edit config.yaml with your settings

# Install services
cp jarvis.service ~/.config/systemd/user/
cp llama-server.service /etc/systemd/system/
systemctl --user daemon-reload
sudo systemctl daemon-reload

# Enable and start
systemctl --user enable --now jarvis
sudo systemctl enable --now llama-server

# Check status
systemctl --user status jarvis
```

### Quick Start
```bash
# Start JARVIS
startjarvis

# Stop JARVIS
stopjarvis

# Restart JARVIS
restartjarvis

# View logs
journalctl --user -u jarvis -f
```

### Basic Usage (Voice)
1. Say "Jarvis" to wake
2. Ask your question naturally
3. JARVIS responds
4. 4-7s adaptive window for follow-up (extends with conversation depth)

### Console Mode
```bash
python3 jarvis_console.py              # Text mode (type commands)
python3 jarvis_console.py --hybrid     # Text input + spoken output
```
Stats panel shows match layer, skill, confidence, timing, and LLM token counts after each command.

**Example Interactions:**
```
You: "Jarvis, what's the weather?"
JARVIS: "Currently 45 degrees and partly cloudy, sir."

You: "How about tomorrow?"
JARVIS: "Tomorrow's forecast shows 52 degrees with scattered showers, sir."

You: "How many lines of code in your codebase?"
JARVIS: "My codebase contains 320,388 lines of Python code across 40 files, sir."
```

---

## ğŸ“š Documentation

- **[SKILL_DEVELOPMENT.md](docs/SKILL_DEVELOPMENT.md)** - How to create skills
- **[DEVELOPMENT.md](docs/DEVELOPMENT.md)** - Development workflows
- **[CHANGELOG.md](CHANGELOG.md)** - Version history
- **[TODO_NEXT_SESSION.md](docs/TODO_NEXT_SESSION.md)** - Current priorities

---

## ğŸ¤ Contributing

JARVIS is a personal project, but ideas and improvements are welcome!

### Adding Skills
1. Read [SKILL_DEVELOPMENT.md](docs/SKILL_DEVELOPMENT.md)
2. Create skill in `skills/` directory
3. Test thoroughly
4. Document in skill README

### Reporting Issues
Include:
- What you said
- What JARVIS responded
- Expected behavior
- Relevant logs

---

## ğŸ“Š Performance Metrics

### Accuracy
- Wake word detection: 100% (Porcupine)
- Speech recognition: 94%+ (fine-tuned Whisper v2, 198 phrases, Southern accent)
- Intent matching: 95%+ (semantic embeddings)
- Routing tests: 38/38 pass (`scripts/test_router.py`)
- Edge case testing: 92.5% (37/40 Phase 1)

### Latency
- Wake word detection: <100ms
- Speech transcription: 0.1-0.2s (GPU-accelerated CTranslate2)
- Intent matching: <100ms (pre-computed semantic embedding cache)
- Skill-handled queries: 300-600ms total
- LLM fallback: 2-4s total (streaming)
- TTS generation: <1s (Kokoro streaming)

### Resource Usage
- RAM: ~4GB (with all models loaded)
- CPU: 10-30% during processing
- GPU: RX 7900 XT via ROCm (STT acceleration)
- Disk: ~15GB (models + code)

---

## ğŸ“ What I've Learned

### Technical Insights
1. **Custom training beats generic models** - 94%+ vs 50% accuracy (198 phrases, 2 training rounds)
2. **REST APIs > subprocess calls** - More reliable for LLM
3. **Semantic matching is powerful** - Reduces pattern count 90%
4. **Preload heavy models** - Prevents audio thread blocking
5. **Log everything** - Makes debugging 10x easier
6. **One router, three frontends** - ConversationRouter eliminates routing duplication across voice/console/web
7. **Prescriptive > permissive for small LLMs** - Explicit numbered rules followed more reliably than prose instructions
8. **Substring `in` for keyword matching is a trap** - `"no" in "diagnostic"` is True. Always use word-boundary matching

### Development Practices
1. **Iterate quickly** - Small changes, frequent testing
2. **Test with real voice** - Keyboard input hides issues
3. **Monitor audio pipeline** - Overflow warnings are critical
4. **Version control everything** - Git saved me multiple times
5. **Document as you go** - Future you will thank you

### Design Decisions
1. **Offline first** - Privacy and reliability
2. **Modular skills** - Easy to extend and maintain
3. **Semantic intents** - Natural language flexibility
4. **British voice** - Character and professionalism
5. **Conservative responses** - Concise, helpful, polite

---

## ğŸ† Achievements

- âœ… Fully functional voice assistant with gapless streaming TTS
- âœ… Custom accent training (fine-tuned Whisper v2, Southern accent, 94%+, 198 phrases)
- âœ… Production-ready event-driven architecture
- âœ… Web research via local LLM tool calling (no cloud required)
- âœ… Conversational memory with semantic recall across sessions
- âœ… Speaker identification and dynamic user profiles
- âœ… 11 modular skills with semantic intent matching (including 16-intent desktop control)
- âœ… Conversational flow engine with persona, state tracking, and shared router
- âœ… Three frontends: voice, console, web UI (all sharing one router)
- âœ… Web Chat UI with streaming, markdown, session sidebar, and health HUD
- âœ… Ambient wake word filter (multi-signal, blocks false triggers)
- âœ… 38 router tests + 92.5% edge case pass rate
- âœ… Hardware failure graceful degradation
- âœ… Sub-600ms skill responses (300-600ms)
- âœ… Open source on GitHub with automated PII redaction

**JARVIS is a legitimate, production-ready AI assistant!**

---

## ğŸ“ Support

For issues or questions:
1. Check logs: `journalctl --user -u jarvis -n 100`
2. Review documentation
3. Test with simple commands first
4. Verify all services running

**Common Issues:**
- **No audio input:** Check microphone permissions
- **No TTS output:** Verify Kokoro/Piper installation
- **Intent not matching:** Lower threshold or add examples
- **LLM not responding:** Check llama-server status

---

**Built with â¤ï¸ and lots of coffee â˜•**

*Built with care, tested obsessively, improved daily.*
